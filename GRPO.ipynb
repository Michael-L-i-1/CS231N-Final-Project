{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Michael-L-i-1/CS231N-Final-Project/blob/main/GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jsUJa5NKH0G2"
      },
      "outputs": [],
      "source": [
        "!pip install hf_xet\n",
        "!pip install trl\n",
        "!pip install peft\n",
        "!pip install flash-attn\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "\n",
        "import torch\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-500M-Instruct\")\n",
        "model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-500M-Instruct\",\n",
        "                                                torch_dtype=torch.bfloat16,\n",
        "                                                _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\").to(DEVICE)\n",
        "model.to('cuda')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Zle_m5GJP207"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qpdawgtqP4Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from transformers.image_utils import load_image\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "import copy\n"
      ],
      "metadata": {
        "id": "R91A7ABbR6e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_drive_path = '/content/drive/My Drive/CS231N Colabs/dataset'\n",
        "json_file_path = os.path.join(base_drive_path, 'metadata.json')\n",
        "images_folder   = os.path.join(base_drive_path, 'images')\n",
        "mini_json_file_path = os.path.join(base_drive_path, 'mini_metadata.json')\n",
        "mini_images_folder   = os.path.join(base_drive_path, 'mini_images')\n",
        "\n",
        "QUESTION = (\n",
        "    \"Given the diagram, list the labels of the circles in order \"\n",
        "    \"from leftmost to rightmost (provide name only).\"\n",
        "    \"You should have all the names included.\"\n",
        ")\n",
        "\n",
        "MODEL_NAME = \"HuggingFaceTB/SmolVLM-500M-Instruct\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "IQG6CoXazTeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRPO Model\n",
        "\n",
        "(NOTE: Consider performance on truncated dataset to really see how much data we need)"
      ],
      "metadata": {
        "id": "-s0AqVibP-n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base model\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "base_model = AutoModelForVision2Seq.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ").to(DEVICE)\n",
        "\n",
        "# reference model\n",
        "ref_model = copy.deepcopy(base_model).eval()"
      ],
      "metadata": {
        "id": "gDdJ_gERQAw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lora confirguation for better RAM usage\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "policy = get_peft_model(base_model, lora_config)\n",
        "policy.generation_config.do_sample = True"
      ],
      "metadata": {
        "id": "GtUDazGFTweX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset class\n",
        "class CircleVlmPromptSet(Dataset):\n",
        "    def __init__(self, meta_json_path, processor, question, base_folder, images_folder):\n",
        "        with open(meta_json_path, 'r') as f:\n",
        "            self.entries = json.load(f)\n",
        "        self.processor = processor\n",
        "        self.question = question\n",
        "        self.base_folder = base_folder\n",
        "        self.images_folder = images_folder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.entries[idx]\n",
        "        img_rel = entry[\"image_path\"]\n",
        "        img_full= os.path.join(self.images_folder, img_rel)\n",
        "        image = Image.open(img_full).convert(\"RGB\")\n",
        "\n",
        "        # responses prompt structure\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\"},\n",
        "                    {\"type\": \"text\", \"text\": self.question}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "        prompt_text = self.processor.apply_chat_template(\n",
        "            messages, add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        # tokenize prompt_text + image into input_ids, attention_mask, pixel_values\n",
        "        model_inputs = self.processor(\n",
        "            text=prompt_text,\n",
        "            images=[image],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=False,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        # build ground-truth labels from metadata\n",
        "        gold_answer = \", \".join(entry[\"order\"])\n",
        "        label_ids   = self.processor.tokenizer(\n",
        "            gold_answer, return_tensors=\"pt\"\n",
        "        ).input_ids.squeeze(0)\n",
        "\n",
        "        # return all fields\n",
        "        return {\n",
        "            \"prompt\": prompt_text,\n",
        "            \"input_ids\": model_inputs[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": model_inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"pixel_values\": model_inputs[\"pixel_values\"].squeeze(0),\n",
        "            \"labels\": label_ids,\n",
        "            \"labels_str\": gold_answer\n",
        "        }\n",
        "\n",
        "def vlm_collate(batch):\n",
        "    # batch is a list of dicts with keys\n",
        "    batch_prompts = [b[\"prompt\"] for b in batch]\n",
        "    batch_labels_str = [b[\"labels_str\"] for b in batch]\n",
        "\n",
        "    # stack pixel_values\n",
        "    pixel_values = torch.stack([b[\"pixel_values\"] for b in batch])\n",
        "\n",
        "    # pad text fields: input_ids, attention_mask, labels → each (B, seq_len)\n",
        "    padded = processor.tokenizer.pad(\n",
        "        {\n",
        "            \"input_ids\": [b[\"input_ids\"] for b in batch],\n",
        "            \"attention_mask\": [b[\"attention_mask\"] for b in batch],\n",
        "            \"labels\": [b[\"labels\"] for b in batch]\n",
        "        },\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # mask padding tokens in labels (so they don’t count toward loss)\n",
        "    padded[\"labels\"][padded[\"labels\"] == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    # build the final batch dict for GRPOTrainer\n",
        "    batch_dict = {\n",
        "        \"prompt\": batch_prompts,\n",
        "        \"input_ids\": padded[\"input_ids\"],\n",
        "        \"attention_mask\": padded[\"attention_mask\"],\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"labels\": padded[\"labels\"],\n",
        "        \"labels_str\": batch_labels_str\n",
        "    }\n",
        "    return batch_dict\n",
        "\n",
        "\n",
        "train_dataset = CircleVlmPromptSet(\n",
        "    meta_json_path=json_file_path,\n",
        "    processor=processor,\n",
        "    question=QUESTION,\n",
        "    base_folder=base_drive_path,\n",
        "    images_folder=images_folder\n",
        ")\n",
        "\n",
        "mini_train_dataset = CircleVlmPromptSet(\n",
        "    meta_json_path=mini_json_file_path,\n",
        "    processor=processor,\n",
        "    question=QUESTION,\n",
        "    base_folder=base_drive_path,\n",
        "    images_folder=mini_images_folder\n",
        ")"
      ],
      "metadata": {
        "id": "B3-N1LycV_S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mini_train_dataset.images_folder)"
      ],
      "metadata": {
        "id": "rz-oLpCT7ZzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_func(prompts, completions, completion_ids, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward = 100 – 10·|length mismatch| – Σ |pred_idx – gold_idx| - Σ penalty_hallucination\n",
        "    higher is better\n",
        "    \"\"\"\n",
        "    MAX_REWARD = 100\n",
        "    PEN_LEN = 10\n",
        "    PEN_HALLUCINATION = 15\n",
        "    gold_list_all = kwargs.get(\"labels_str\", [])\n",
        "    rewards = []\n",
        "\n",
        "    for comp, gold_str in zip(completions, gold_list_all):\n",
        "        # parse lists\n",
        "        trial   = [n.strip() for n in comp.split(\"Assistant:\")[-1].split(\",\") if n.strip()]\n",
        "        correct = [n.strip() for n in gold_str.split(\",\") if n.strip()]\n",
        "\n",
        "        # base reward = 100 – length penalty\n",
        "        reward  = MAX_REWARD - abs(len(correct) - len(trial)) * PEN_LEN\n",
        "        if not correct:                       # degenerate case\n",
        "            rewards.append(float(MAX_REWARD if not trial else max(0, reward)))\n",
        "            continue\n",
        "\n",
        "        # order penalty\n",
        "        unmatched_gold = list(correct)\n",
        "        gold_pos = {name: idx for idx, name in enumerate(correct)}\n",
        "\n",
        "        for t_idx, t_name in enumerate(trial):\n",
        "            if t_name in gold_pos and t_name in unmatched_gold:\n",
        "                reward -= abs(t_idx - gold_pos[t_name])\n",
        "                unmatched_gold.remove(t_name)\n",
        "            # hallucination penalty\n",
        "            elif t_name not in correct:\n",
        "                 reward -= PEN_HALLUCINATION\n",
        "\n",
        "\n",
        "        rewards.append(float(reward))\n",
        "    return rewards"
      ],
      "metadata": {
        "id": "DVHi3G72WYB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grpo_cfg = GRPOConfig(\n",
        "    num_generations=8,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "    beta=0.001,\n",
        "    optim=\"adamw_8bit\",\n",
        "    bf16=True,\n",
        "    max_completion_length=64,\n",
        "    num_train_epochs=1,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=[\"wandb\"],\n",
        "    logging_steps=1,\n",
        "    logging_dir=\"SmolVLM_logs\",\n",
        "    output_dir=\"SmolVLM_output\",\n",
        "\n",
        "    temperature=1,\n",
        "    top_k=5,\n",
        "    top_p=0.8,\n",
        ")"
      ],
      "metadata": {
        "id": "-HHRaXafWb5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model=policy,\n",
        "    args=grpo_cfg,\n",
        "    train_dataset=mini_train_dataset,\n",
        "    reward_funcs=simple_reward_func,\n",
        ")"
      ],
      "metadata": {
        "id": "kG8Sr2VoXPJ2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "HOIbq-_EuWpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"GRPO\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "D_-OsCEKZC0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"SmolVLM_finetuned/\")\n",
        "processor.save_pretrained(\"SmolVLM_finetuned/\")"
      ],
      "metadata": {
        "id": "Qm976DYxzKuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "B5T82F7v1y_H"
      },
      "source": [
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "\n",
        "# load the saved model and processor\n",
        "model_path = \"SmolVLM_finetuned/\"\n",
        "processor = AutoProcessor.from_pretrained(model_path)\n",
        "model = AutoModelForVision2Seq.from_pretrained(model_path)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXgL_uI02J-p"
      },
      "source": [
        "# TESTING FINETUNED MODELx\n",
        "\n",
        "mage_path = '/content/test.png'\n",
        "\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "prompt = (\n",
        "    \"Given the diagram, list the labels of the circles in order \"\n",
        "    \"from leftmost to rightmost (provide name only).\"\n",
        "    \"You should have all the names included.\"\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": prompt}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "prompt_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "inputs = processor(text=prompt_text, images=image, return_tensors=\"pt\")\n",
        "\n",
        "# move inputs to gpu\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "# generate a prediction\n",
        "with torch.no_grad():\n",
        "    output = model.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=1.0)\n",
        "\n",
        "# decode the output\n",
        "predicted_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"Predicted order:\", predicted_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}