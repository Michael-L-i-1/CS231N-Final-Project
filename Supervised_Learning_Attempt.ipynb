{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Michael-L-i-1/CS231N-Final-Project/blob/main/Supervised_Learning_Attempt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuCy0FlUTdbf"
      },
      "source": [
        "# Load Model\n",
        "\n",
        "We will be using SmolVLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pnITa0s9zWsz"
      },
      "outputs": [],
      "source": [
        "!pip install hf_xet\n",
        "!pip install flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GLErB_vyzBj4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "import torch\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-256M-Instruct\")\n",
        "model     = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-256M-Instruct\",\n",
        "                                                torch_dtype=torch.bfloat16,\n",
        "                                                _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\").to(DEVICE)\n",
        "model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx33qgqYXYxP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXkHHJAZTbMM"
      },
      "source": [
        "# Test Single Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OBGe24_0ypl"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from transformers.image_utils import load_image\n",
        "\n",
        "\n",
        "# load test image\n",
        "image = Image.open(\"/content/test.png\")\n",
        "\n",
        "question = \"\"\"Given the diagram, list the labels of the circles in order from leftmost to rightmost\n",
        "          (provide name only)\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": question}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# prepare inputs\n",
        "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
        "inputs = inputs.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvAXDA1_06CT"
      },
      "outputs": [],
      "source": [
        "# generate outputs\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
        "generated_texts = processor.batch_decode(\n",
        "    generated_ids,\n",
        "    skip_special_tokens=True,\n",
        ")\n",
        "\n",
        "print(generated_texts[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6mOWIl4TlL5"
      },
      "source": [
        "# Evaluating Baseline on Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwV9UqTPTn4K"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "base_drive_path = '/content/drive/My Drive/CS231N Colabs/dataset'\n",
        "json_file_path = os.path.join(base_drive_path, 'metadata.json')\n",
        "\n",
        "# load in the dataset\n",
        "with open(json_file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "count = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# process all the images\n",
        "for entry in tqdm(data, desc=\"Processing Images\"):\n",
        "  count += 1\n",
        "  if count > 250:\n",
        "      break\n",
        "  image_relative_path = entry['image_path']\n",
        "  image_full_path = os.path.join('/content/drive/My Drive/CS231N Colabs', image_relative_path)\n",
        "\n",
        "  image = Image.open(image_full_path)\n",
        "\n",
        "  question = \"\"\"Given the diagram, list the labels of the circles in order from leftmost to rightmost\n",
        "            (provide name only)\"\"\"\n",
        "\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": [\n",
        "              {\"type\": \"image\"},\n",
        "              {\"type\": \"text\", \"text\": question}\n",
        "          ]\n",
        "      },\n",
        "  ]\n",
        "\n",
        "  # prepare inputs\n",
        "  prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "  inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
        "  inputs = inputs.to(DEVICE)\n",
        "\n",
        "  # generate outputs\n",
        "  with torch.no_grad():\n",
        "      generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
        "  generated_texts = processor.batch_decode(\n",
        "      generated_ids,\n",
        "      skip_special_tokens=True,\n",
        "  )\n",
        "\n",
        "  # process the output\n",
        "  predicted_order = generated_texts[0].strip()\n",
        "  predicted_order = predicted_order.split(\"Assistant:\")[-1].strip()\n",
        "  predicted_order = [name.strip() for name in predicted_order.split(\",\")]\n",
        "\n",
        "  expected_order = entry['order']\n",
        "\n",
        "  if predicted_order == expected_order:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "\n",
        "print(f\"Accuracy: {correct / total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WwzTrBuXU1bR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# If you have a `dataset/` folder, list that too\n",
        "dataset_path = '/content/drive/My Drive/CS231N Colabs/dataset'\n",
        "os.listdir(dataset_path)\n",
        "print(\"image_0.png\" in os.listdir(dataset_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geCuQuBdKc1l"
      },
      "source": [
        "# Baseline Supervised Fine Tuning w/ Cross Entropy Loss\n",
        "\n",
        "CS231N Colabs/dataset - 2500 images\n",
        "2000 train, 250 val, 250 test\n",
        "\n",
        "0-1999, 2000-2249, 2250-2499\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assume `processor` and `model` are already defined (and moved to DEVICE).\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#######################################\n",
        "# 1) Define a custom Dataset class  ##\n",
        "#######################################\n",
        "\n",
        "class VLMOrderDataset(Dataset):\n",
        "    def __init__(self, metadata_json_path, base_image_dir, processor, max_length=128):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            metadata_json_path (str): path to metadata.json\n",
        "            base_image_dir (str): root folder where images live\n",
        "            processor (AutoProcessor): the Hugging Face processor for SmolVLM\n",
        "            max_length (int): maximum token length for textual sequences\n",
        "        \"\"\"\n",
        "        with open(metadata_json_path, 'r') as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        self.base_image_dir = base_image_dir\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Fixed question text (same for every example):\n",
        "        self.question_text = (\n",
        "            \"Given the diagram, list the labels of the circles in order from leftmost to rightmost \"\n",
        "            \"(provide name only).\"\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "        image_rel_path = entry[\"image_path\"]\n",
        "        image_rel_path = image_rel_path.replace(\"dataset/\", \"\")\n",
        "        order_list = entry[\"order\"]  # e.g. [\"Hannah\",\"Grace\",\"Alice\",\"Emily\",\"David\"]\n",
        "\n",
        "        # 1) Load the image from disk:\n",
        "        image_path = os.path.join(self.base_image_dir, image_rel_path)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # 2) Build the “prompt” exactly as at inference:\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\"},\n",
        "                    {\"type\": \"text\", \"text\": self.question_text}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "        prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "        # 3) Convert the target list into a comma‐separated string:\n",
        "        #    e.g. \"Hannah, Grace, Alice, Emily, David\"\n",
        "        target_text = \", \".join(order_list)\n",
        "\n",
        "        # 4) Tokenize both with truncation disabled so that the <image> token is never dropped:\n",
        "        encoding = self.processor(\n",
        "            text=prompt,\n",
        "            images=[image],\n",
        "            text_target=target_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=False,       # ← disable truncation for the input\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "        # encoding[\"input_ids\"] is shape (1, seq_len), same for attention_mask & labels;\n",
        "        # encoding[\"pixel_values\"] is shape (1, 3, H, W).\n",
        "        # We .squeeze(0) to make them 1D/3D instead of batch‐size=1.\n",
        "        pixel_values = encoding[\"pixel_values\"].squeeze(0)     # (3, H, W)\n",
        "        input_ids     = encoding[\"input_ids\"].squeeze(0)       # (seq_len,)\n",
        "        attention_mask= encoding[\"attention_mask\"].squeeze(0)  # (seq_len,)\n",
        "        labels        = encoding[\"labels\"].squeeze(0)          # (target_seq_len,)\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"input_ids\":     input_ids,\n",
        "            \"attention_mask\":attention_mask,\n",
        "            \"labels\":        labels\n",
        "        }\n",
        "\n",
        "###############################################\n",
        "# 2) Instantiate train/val splits & DataLoaders ##\n",
        "###############################################\n",
        "\n",
        "# Paths—adjust to wherever you mounted your drive in Colab:\n",
        "base_drive_path   = \"/content/drive/My Drive/CS231N Colabs\"\n",
        "metadata_path     = os.path.join(base_drive_path, \"dataset/metadata.json\")\n",
        "images_base_dir = os.path.join(base_drive_path, \"dataset\", \"images\")\n",
        "\n",
        "# Load full dataset\n",
        "full_dataset = VLMOrderDataset(\n",
        "    metadata_json_path=metadata_path,\n",
        "    base_image_dir=images_base_dir,\n",
        "    processor=processor,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "# Split indices: 0–1999 = train, 2000–2249 = val, 2250–2499 = test\n",
        "num_examples = len(full_dataset)  # should be 2500\n",
        "assert num_examples == 2500, \"Check that metadata.json has 2500 entries.\"\n",
        "\n",
        "train_indices = list(range(0, 2000))\n",
        "val_indices   = list(range(2000, 2250))\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "train_dataset = Subset(full_dataset, train_indices)\n",
        "val_dataset   = Subset(full_dataset, val_indices)\n",
        "\n",
        "# DataLoader (you can adjust batch_size as GPU allows; 4–8 is a good starting point)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "###################################################\n",
        "# 3) Set up optimizer, training loop, and metrics ##\n",
        "###################################################\n",
        "\n",
        "#  We’ll use AdamW and a small learning rate. Feel free to tune later.\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "num_epochs = 3   # start small; you can increase if you have time\n",
        "\n",
        "# (Optional) learning‐rate scheduler, gradient clipping, etc. can be added here.\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_steps = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} / {num_epochs} (train)\", leave=False)\n",
        "    for batch in pbar:\n",
        "        # Move everything to DEVICE\n",
        "        pixel_values  = batch[\"pixel_values\"].to(DEVICE)      # (B, 3, H, W)\n",
        "        input_ids      = batch[\"input_ids\"].to(DEVICE)        # (B, seq_len)\n",
        "        attention_mask = batch[\"attention_mask\"].to(DEVICE)   # (B, seq_len)\n",
        "        labels         = batch[\"labels\"].to(DEVICE)           # (B, target_len)\n",
        "\n",
        "        # Forward pass: since we passed `labels`, HF will compute cross‐entropy internally\n",
        "        outputs = model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # (Optional) torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_steps += 1\n",
        "        pbar.set_postfix_str(f\"loss={train_loss/train_steps:.4f}\")\n",
        "\n",
        "    avg_train_loss = train_loss / train_steps\n",
        "    print(f\"\\nEpoch {epoch+1} — Avg train loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 4) Validate after each epoch\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_steps = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} / {num_epochs} (val)\", leave=False):\n",
        "            pixel_values  = batch[\"pixel_values\"].to(DEVICE)\n",
        "            input_ids      = batch[\"input_ids\"].to(DEVICE)\n",
        "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "            labels         = batch[\"labels\"].to(DEVICE)\n",
        "\n",
        "            outputs = model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "            )\n",
        "            val_loss += outputs.loss.item()\n",
        "            val_steps += 1\n",
        "\n",
        "    avg_val_loss = val_loss / val_steps\n",
        "    print(f\"Epoch {epoch+1} — Avg val loss: {avg_val_loss:.4f}\\n\")\n",
        "\n",
        "    # (Optional) Save the best‐performing checkpoint\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        save_path = \"./best_smolvlm_baseline.pt\"\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"  → New best model saved at epoch {epoch+1} (val loss {avg_val_loss:.4f})\\n\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "id": "EcOhlnpWOULL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkMCJX-JKcGw"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install dependencies (if not already installed)\n",
        "!pip install --quiet hf_xet flash-attn\n",
        "!pip install --quiet transformers torch pillow tqdm\n",
        "!pip install --quiet torch torchvision transformers pillow tqdm\n",
        "!pip install --quiet --upgrade torch torchvision transformers pillow tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from torch.optim import AdamW\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "1EeQwtytIvNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and Paths\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "\n",
        "# Device selection\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ----------------------------\n",
        "# Adjust these two paths:\n",
        "# ----------------------------\n",
        "# 1) Where your images live (e.g. image_0.png, image_1.png, ...)\n",
        "IMAGE_DIR = \"/content/drive/My Drive/CS231N Colabs/dataset\"\n",
        "\n",
        "# 2) Where your metadata.json lives\n",
        "METADATA_PATH = \"/content/drive/My Drive/CS231N Colabs/dataset/metadata.json\"\n",
        "\n",
        "# # 3) (Optional) A place to save checkpoints\n",
        "# CHECKPOINT_DIR = \"/content/checkpoints\"\n",
        "# os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Print confirmations\n",
        "print(\"Device:\", DEVICE)\n",
        "print(\"Image directory:\", IMAGE_DIR)\n",
        "print(\"Metadata path:\", METADATA_PATH)\n"
      ],
      "metadata": {
        "id": "40fIKGqUQTdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 – Load the pre‐trained processor & model\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
        "model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\").to(DEVICE)\n",
        "print(\"✓ Loaded SmolVLM‐Instruct\")\n"
      ],
      "metadata": {
        "id": "LYo8O6M8QawX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 – Define Dataset that returns raw PIL images + chat‐prompt + target string\n",
        "class CircleOrderDataset(Dataset):\n",
        "    def __init__(self, metadata_list, image_dir, processor):\n",
        "        self.entries   = metadata_list\n",
        "        self.image_dir = image_dir\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.entries[idx]\n",
        "\n",
        "        # 1) Load image\n",
        "        img_name = os.path.basename(entry[\"image_path\"])\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image    = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # 2) Build chat‐style prompt (one <image> token)\n",
        "        question = \"Given the diagram, list the labels of the circles in order from leftmost to rightmost.\"\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\"},\n",
        "                    {\"type\": \"text\", \"text\": question}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "        chat_prompt = self.processor.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "\n",
        "        # 3) Build target (comma‐separated)\n",
        "        target_text = \", \".join(entry[\"order\"])\n",
        "\n",
        "        return {\n",
        "            \"image\":       image,\n",
        "            \"chat_prompt\": chat_prompt,  # contains one <image> token\n",
        "            \"target_text\": target_text\n",
        "        }\n"
      ],
      "metadata": {
        "id": "uy7EtOw7Qd-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 – Load metadata.json & split into train/val/test lists\n",
        "with open(METADATA_PATH, \"r\") as f:\n",
        "    all_metadata = json.load(f)\n",
        "\n",
        "assert len(all_metadata) == 2500, f\"Expected 2500 entries, got {len(all_metadata)}\"\n",
        "\n",
        "# Split indices\n",
        "train_metadata = all_metadata[0:2000]    # 2000 for training\n",
        "val_metadata   = all_metadata[2000:2250] # 250 for validation\n",
        "test_metadata  = all_metadata[2250:2500] # 250 for testing\n",
        "\n",
        "# Instantiate Datasets\n",
        "train_dataset = CircleOrderDataset(train_metadata, IMAGE_DIR, processor)\n",
        "val_dataset   = CircleOrderDataset(val_metadata,   IMAGE_DIR, processor)\n",
        "test_dataset  = CircleOrderDataset(test_metadata,  IMAGE_DIR, processor)\n",
        "\n",
        "print(\"✓ Train size:\", len(train_dataset))\n",
        "print(\"✓ Val   size:\", len(val_dataset))\n",
        "print(\"✓ Test  size:\", len(test_dataset))\n"
      ],
      "metadata": {
        "id": "dk66X28QQh2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_collator(batch):\n",
        "    \"\"\"\n",
        "    batch: list of dicts, each with keys:\n",
        "      - \"image\"       : PIL.Image\n",
        "      - \"chat_prompt\" : str (contains exactly one <image> token)\n",
        "      - \"target_text\" : str (comma‐separated labels)\n",
        "\n",
        "    Returns:\n",
        "      {\n",
        "        pixel_values:   Tensor (B, 1, 3, H, W),\n",
        "        input_ids:      Tensor (B, L1),\n",
        "        attention_mask: Tensor (B, L1),\n",
        "        labels:         Tensor (B, L2) with pad_token_id → -100\n",
        "      }\n",
        "    \"\"\"\n",
        "    images       = [item[\"image\"]       for item in batch]\n",
        "    chat_prompts = [item[\"chat_prompt\"] for item in batch]\n",
        "    targets      = [item[\"target_text\"] for item in batch]\n",
        "\n",
        "    # 1) Multimodal tokenization: text with <image> + images\n",
        "    model_inputs = processor(\n",
        "        text=chat_prompts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,       # pad to longest in batch\n",
        "        truncation=True     # truncate if too long\n",
        "    )\n",
        "    # model_inputs.pixel_values has shape (B, 1, C, H, W)—do NOT squeeze!\n",
        "\n",
        "    pixel_values   = model_inputs.pixel_values    # (B, 1, 3, H, W)\n",
        "    input_ids      = model_inputs.input_ids       # (B, L1)\n",
        "    attention_mask = model_inputs.attention_mask  # (B, L1)\n",
        "\n",
        "    # 2) Tokenize & pad the target strings\n",
        "    label_encodings = processor.tokenizer(\n",
        "        targets,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )[\"input_ids\"]  # (B, L2)\n",
        "\n",
        "    # 3) Replace pad_token_id with -100 so loss ignores those positions\n",
        "    pad_id = processor.tokenizer.pad_token_id\n",
        "    label_encodings[label_encodings == pad_id] = -100\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\":   pixel_values.to(DEVICE),     # (B, 1, 3, H, W)\n",
        "        \"input_ids\":      input_ids.to(DEVICE),        # (B, L1)\n",
        "        \"attention_mask\": attention_mask.to(DEVICE),   # (B, L1)\n",
        "        \"labels\":         label_encodings.to(DEVICE),  # (B, L2)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "AfifDWseRDnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 – Create DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "print(\"✓ # train batches:\", len(train_loader))\n",
        "print(\"✓ # val   batches:\", len(val_loader))\n"
      ],
      "metadata": {
        "id": "MQroHQLzRF2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Immediately after loading processor, shrink images to 128×128\n",
        "processor.image_processor.size = {\"height\": 128, \"width\": 128}\n",
        "\n",
        "# 2) Load the model and enable checkpointing\n",
        "model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\").to(DEVICE)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# 3) In your DataLoaders, use batch_size=1\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=1, shuffle=False, collate_fn=data_collator)\n",
        "\n",
        "# 4) Use mixed precision + smaller prompt length in collator\n",
        "#    (modify your collator to include max_length=128 if desired)\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "scaler = GradScaler()\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            outputs = model(\n",
        "                pixel_values=batch[\"pixel_values\"],\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                labels=batch[\"labels\"],\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} train loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    total_val = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "            with autocast():\n",
        "                outputs = model(\n",
        "                    pixel_values=batch[\"pixel_values\"],\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    labels=batch[\"labels\"],\n",
        "                )\n",
        "                total_val += outputs.loss.item()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} val   loss: {total_val/len(val_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "RdSnRhUhRLF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# (Assume processor, model, train_loader, val_loader, optimizer, etc. are already defined)\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(3):\n",
        "    # ----- Training -----\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 1) Move all inputs to the same device\n",
        "        pixel_values   = batch[\"pixel_values\"].to(DEVICE)\n",
        "        input_ids      = batch[\"input_ids\"].to(DEVICE)\n",
        "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "        labels         = batch[\"labels\"].to(DEVICE)\n",
        "\n",
        "        # 2) Now run the forward pass under autocast\n",
        "        with autocast():\n",
        "            outputs = model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 2) Delete intermediate tensors to break Python references\n",
        "        del outputs, loss\n",
        "\n",
        "        # 3) Force garbage collection, then free any unused cached GPU memory\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # ----- Validation -----\n",
        "    model.eval()\n",
        "    total_val = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in val_loader:\n",
        "          pixel_values   = batch[\"pixel_values\"].to(DEVICE)\n",
        "          input_ids      = batch[\"input_ids\"].to(DEVICE)\n",
        "          attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "          labels         = batch[\"labels\"].to(DEVICE)\n",
        "\n",
        "          with autocast():\n",
        "              outputs = model(\n",
        "                  pixel_values=pixel_values,\n",
        "                  input_ids=input_ids,\n",
        "                  attention_mask=attention_mask,\n",
        "                  labels=labels,\n",
        "              )\n",
        "              val_loss = outputs.loss\n",
        "\n",
        "    avg_val_loss = total_val / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1} — Avg Val Loss: {avg_val_loss:.4f}\\n\")\n",
        "\n",
        "    # Optionally, you can also call empty_cache once more here:\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "RXBriGuiahfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell X: Verify that all inputs and model parameters are on the same device\n",
        "\n",
        "import torch\n",
        "\n",
        "# Grab one batch from the training DataLoader\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "# 1) Print devices of raw batch tensors\n",
        "print(\"Raw batch tensor devices:\")\n",
        "print(\"  pixel_values:   \", batch[\"pixel_values\"].device)\n",
        "print(\"  input_ids:      \", batch[\"input_ids\"].device)\n",
        "print(\"  attention_mask: \", batch[\"attention_mask\"].device)\n",
        "print(\"  labels:         \", batch[\"labels\"].device)\n",
        "\n",
        "# 2) Move each tensor to the target DEVICE\n",
        "pixel_values   = batch[\"pixel_values\"].to(DEVICE)\n",
        "input_ids      = batch[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "labels         = batch[\"labels\"].to(DEVICE)\n",
        "\n",
        "# 3) Print devices after .to(DEVICE)\n",
        "print(\"\\nAfter .to(DEVICE):\")\n",
        "print(\"  pixel_values:   \", pixel_values.device)\n",
        "print(\"  input_ids:      \", input_ids.device)\n",
        "print(\"  attention_mask: \", attention_mask.device)\n",
        "print(\"  labels:         \", labels.device)\n",
        "\n",
        "# 4) Check every model parameter’s device\n",
        "print(\"\\nModel parameter devices (non-GPU parameters, if any):\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.device != torch.device(DEVICE):\n",
        "        print(f\"  ⚠️  {name} is on {param.device}\")\n"
      ],
      "metadata": {
        "id": "8c1onDCHXj1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell Z: Move the entire model (all submodules) to GPU\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Verify that no parameters remain on CPU\n",
        "import torch\n",
        "for name, param in model.named_parameters():\n",
        "    if param.device != torch.device(DEVICE):\n",
        "        print(f\"⚠️ {name} is still on {param.device}\")\n",
        "\n",
        "print(\"✓ All model parameters are now on\", DEVICE)\n"
      ],
      "metadata": {
        "id": "ohMnUNHpW-0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "\n",
        "# 1) Force‐free any leftover GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# 2) Check that GPU is truly empty\n",
        "print(\"Free CUDA memory before loading:\",\n",
        "      torch.cuda.mem_get_info()[0] / (1024**3), \"GB\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 3) Load processor and shrink images to 32×32 (tiny)\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
        "processor.image_processor.size = {\"height\": 32, \"width\": 32}\n",
        "\n",
        "# 4) Load model in FP16, enable checkpointing, and move to GPU\n",
        "model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\").half().to(DEVICE)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# 5) Immediately delete everything except the model and processor\n",
        "del processor\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 6) Check GPU usage now\n",
        "print(\"Free CUDA memory after loading:\",\n",
        "      torch.cuda.mem_get_info()[0] / (1024**3), \"GB\")\n"
      ],
      "metadata": {
        "id": "I-zcQhD6cTc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, uninstall any existing flash‐attn that may be incompatible:\n",
        "!pip uninstall -y flash-attn\n",
        "\n",
        "# Clone the official FlashAttention repo and build it against the current PyTorch:\n",
        "!git clone https://github.com/Dao-AILab/flash-attention.git\n",
        "%cd flash-attention\n",
        "!pip install .\n",
        "\n",
        "# Go back to your notebook root and clear cache\n",
        "%cd ..\n",
        "import torch, gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Now retry loading SmolVLM\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
        "processor.image_processor.size = {\"height\": 64, \"width\": 64}\n",
        "\n",
        "model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\").half().to(DEVICE)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"✅ FlashAttention built and model loaded on:\", DEVICE)\n"
      ],
      "metadata": {
        "id": "hdH-6sDRdQx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Train SmolVLM‐Instruct on CPU only (no CUDA)\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "\n",
        "# 1) Force everything onto CPU\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "# 2) Load the processor and do a small image resize (e.g. 128×128)\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
        "processor.image_processor.size = {\"height\": 128, \"width\": 128}\n",
        "\n",
        "# 3) Define Dataset (returns PIL images, chat‐style prompts, target texts)\n",
        "class CircleOrderDataset(Dataset):\n",
        "    def __init__(self, metadata_list, image_dir, processor):\n",
        "        self.entries   = metadata_list\n",
        "        self.image_dir = image_dir\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.entries[idx]\n",
        "        img_name = os.path.basename(entry[\"image_path\"])\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image    = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        question = \"Given the diagram, list the labels of the circles in order from leftmost to rightmost.\"\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": question}]}\n",
        "        ]\n",
        "        chat_prompt = self.processor.apply_chat_template(messages, add_generation_prompt=False)\n",
        "        target_text = \", \".join(entry[\"order\"])\n",
        "        return {\"image\": image, \"chat_prompt\": chat_prompt, \"target_text\": target_text}\n",
        "\n",
        "# 4) Data collator: tokenize on CPU (no .to(cuda))\n",
        "def data_collator(batch):\n",
        "    images       = [item[\"image\"] for item in batch]\n",
        "    chat_prompts = [item[\"chat_prompt\"] for item in batch]\n",
        "    targets      = [item[\"target_text\"] for item in batch]\n",
        "\n",
        "    # Encode text+image together\n",
        "    model_inputs = processor(\n",
        "        text=chat_prompts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "    # model_inputs.pixel_values has shape (B, 1, 3, 128, 128) on CPU\n",
        "    pixel_values   = model_inputs.pixel_values   # (B,1,3,128,128)\n",
        "    input_ids      = model_inputs.input_ids      # (B, L1)\n",
        "    attention_mask = model_inputs.attention_mask # (B, L1)\n",
        "\n",
        "    # Tokenize and pad targets\n",
        "    label_encodings = processor.tokenizer(\n",
        "        targets,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )[\"input_ids\"]  # (B, L2)\n",
        "    pad_id = processor.tokenizer.pad_token_id\n",
        "    label_encodings[label_encodings == pad_id] = -100\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\":   pixel_values,\n",
        "        \"input_ids\":      input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\":         label_encodings,\n",
        "    }\n",
        "\n",
        "# 5) Load metadata.json and split\n",
        "BASE_DIR     = \"/content\"                         # adjust if needed\n",
        "IMAGE_DIR    = os.path.join(BASE_DIR, \"dataset\")  # folder with images\n",
        "METADATA_PATH = os.path.join(BASE_DIR, \"metadata.json\")\n",
        "\n",
        "with open(METADATA_PATH, \"r\") as f:\n",
        "    all_metadata = json.load(f)\n",
        "\n",
        "train_meta = all_metadata[0:2000]\n",
        "val_meta   = all_metadata[2000:2250]\n",
        "\n",
        "train_dataset = CircleOrderDataset(train_meta, IMAGE_DIR, processor)\n",
        "val_dataset   = CircleOrderDataset(val_meta,   IMAGE_DIR, processor)\n",
        "\n",
        "# 6) DataLoaders with batch_size=1 (keep small to reduce CPU RAM)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=1, shuffle=False, collate_fn=data_collator)\n",
        "\n",
        "print(\"✓ Train batches:\", len(train_loader), \"| Val batches:\", len(val_loader))\n",
        "\n",
        "# 7) Load model on CPU and set up optimizer + loss\n",
        "model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\").to(DEVICE)\n",
        "model.gradient_checkpointing_enable()  # saves some CPU activation memory\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn   = CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "# 8) Simple training loop on CPU\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
        "        # Move inputs to CPU (they are already on CPU by default)\n",
        "        pixel_values   = batch[\"pixel_values\"]      # (1,1,3,128,128)\n",
        "        input_ids      = batch[\"input_ids\"]         # (1, L1)\n",
        "        attention_mask = batch[\"attention_mask\"]    # (1, L1)\n",
        "        labels         = batch[\"labels\"]            # (1, L2)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        # HuggingFace’s Vision2Seq returns outputs.loss when labels are provided\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} ▶ Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 9) Validation on CPU\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
        "            pixel_values   = batch[\"pixel_values\"]\n",
        "            input_ids      = batch[\"input_ids\"]\n",
        "            attention_mask = batch[\"attention_mask\"]\n",
        "            labels         = batch[\"labels\"]\n",
        "\n",
        "            outputs = model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            total_val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1} ▶ Avg Val Loss: {avg_val_loss:.4f}\\n\")\n"
      ],
      "metadata": {
        "id": "xKKdrbKufoIW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}