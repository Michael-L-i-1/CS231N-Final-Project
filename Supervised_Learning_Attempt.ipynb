{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Michael-L-i-1/CS231N-Final-Project/blob/main/Supervised_Learning_Attempt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuCy0FlUTdbf"
      },
      "source": [
        "# Load Model\n",
        "\n",
        "We will be using SmolVLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pnITa0s9zWsz",
        "outputId": "f21c7b4d-2eec-475e-9f4d-a1bec8b1ce83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hf_xet in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->flash-attn)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->flash-attn)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->flash-attn)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->flash-attn)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->flash-attn)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->flash-attn)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->flash-attn)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m133.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187831595 sha256=58853b28a5a926cae14402bfd8d4d93a45ebf8f9e79533f37ab09d0d77a99c05\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, flash-attn\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed flash-attn-2.7.4.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install hf_xet\n",
        "!pip install flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GLErB_vyzBj4",
        "outputId": "c6086773-c98e-46fe-abab-5a2c20a6c917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Idefics3ForConditionalGeneration(\n",
              "  (model): Idefics3Model(\n",
              "    (vision_model): Idefics3VisionTransformer(\n",
              "      (embeddings): Idefics3VisionEmbeddings(\n",
              "        (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
              "        (position_embedding): Embedding(1024, 768)\n",
              "      )\n",
              "      (encoder): Idefics3Encoder(\n",
              "        (layers): ModuleList(\n",
              "          (0-11): 12 x Idefics3EncoderLayer(\n",
              "            (self_attn): Idefics3VisionAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): Idefics3VisionMLP(\n",
              "              (activation_fn): PytorchGELUTanh()\n",
              "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "    )\n",
              "    (connector): Idefics3Connector(\n",
              "      (modality_projection): Idefics3SimpleMLP(\n",
              "        (proj): Linear(in_features=12288, out_features=576, bias=False)\n",
              "      )\n",
              "    )\n",
              "    (text_model): LlamaModel(\n",
              "      (embed_tokens): Embedding(49280, 576, padding_idx=2)\n",
              "      (layers): ModuleList(\n",
              "        (0-29): 30 x LlamaDecoderLayer(\n",
              "          (self_attn): LlamaAttention(\n",
              "            (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
              "            (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
              "            (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
              "            (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
              "          )\n",
              "          (mlp): LlamaMLP(\n",
              "            (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
              "            (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
              "            (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
              "            (act_fn): SiLU()\n",
              "          )\n",
              "          (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "          (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "        )\n",
              "      )\n",
              "      (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "      (rotary_emb): LlamaRotaryEmbedding()\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=576, out_features=49280, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "import torch\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-256M-Instruct\")\n",
        "model     = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-256M-Instruct\",\n",
        "                                                torch_dtype=torch.bfloat16,\n",
        "                                                _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\").to(DEVICE)\n",
        "model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx33qgqYXYxP",
        "outputId": "a7782418-cb3f-4fbf-9a19-3388635df330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXkHHJAZTbMM"
      },
      "source": [
        "# Test Single Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OBGe24_0ypl"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from transformers.image_utils import load_image\n",
        "\n",
        "\n",
        "# load test image\n",
        "image = Image.open(\"/content/test.png\")\n",
        "\n",
        "question = \"\"\"Given the diagram, list the labels of the circles in order from leftmost to rightmost\n",
        "          (provide name only)\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": question}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# prepare inputs\n",
        "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
        "inputs = inputs.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvAXDA1_06CT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3de93b1-0175-4745-9e03-309ddc413784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Given the diagram, list the labels of the circles in order from leftmost to rightmost\n",
            "          (provide name only)\n",
            "Assistant: Alice, Emily, Grace, David\n"
          ]
        }
      ],
      "source": [
        "# generate outputs\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
        "generated_texts = processor.batch_decode(\n",
        "    generated_ids,\n",
        "    skip_special_tokens=True,\n",
        ")\n",
        "\n",
        "print(generated_texts[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6mOWIl4TlL5"
      },
      "source": [
        "# Evaluating Baseline on Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "538c7c2b14264201b480c558f10518a4",
            "4dc5c25d9f074345ae90958de28bd0b4",
            "e4028365966c4825910a6093849d9e3c",
            "834d437f8a6b4fdca7be2308c13bb6a9",
            "14d0b67f3ab0402cba972afd1eacc8fc",
            "5ae7ecc881854dad8498b6196e1c56bd",
            "df1b32aa29764b69b1e36795b833954e",
            "5843d952c6b94746b30bd72525a87a07",
            "5747eebb43664ce6984d9db2f4dede6f",
            "bb00eec0cfb3461192e2546ae6147973",
            "d81ef8e1b1d94f1c940af6eab73f5480"
          ]
        },
        "id": "TwV9UqTPTn4K",
        "outputId": "8309f115-a1d7-40a5-e832-eb85067918fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Images:   0%|          | 0/2500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "538c7c2b14264201b480c558f10518a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n",
            "Warning: Image not found at /content/drive/My Drive/CS231N Colabs/dataset/images/dataset/image_249.png. Skipping.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2b631a8f066f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0;31m# print(correct)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy: {correct / total}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "base_drive_path = '/content/drive/My Drive/CS231N Colabs/dataset'\n",
        "json_file_path = os.path.join(base_drive_path, 'metadata.json')\n",
        "\n",
        "# load in the dataset\n",
        "with open(json_file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "count = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# process all the images\n",
        "for entry in tqdm(data, desc=\"Processing Images\"):\n",
        "  count += 1\n",
        "  if count > 250:\n",
        "      break\n",
        "  image_relative_path = entry['image_path']\n",
        "  image_full_path = os.path.join('/content/drive/My Drive/CS231N Colabs', image_relative_path)\n",
        "\n",
        "  image = Image.open(image_full_path)\n",
        "\n",
        "  question = \"\"\"Given the diagram, list the labels of the circles in order from leftmost to rightmost\n",
        "            (provide name only)\"\"\"\n",
        "\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": [\n",
        "              {\"type\": \"image\"},\n",
        "              {\"type\": \"text\", \"text\": question}\n",
        "          ]\n",
        "      },\n",
        "  ]\n",
        "\n",
        "  # prepare inputs\n",
        "  prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "  inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
        "  inputs = inputs.to(DEVICE)\n",
        "\n",
        "  # generate outputs\n",
        "  with torch.no_grad():\n",
        "      generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
        "  generated_texts = processor.batch_decode(\n",
        "      generated_ids,\n",
        "      skip_special_tokens=True,\n",
        "  )\n",
        "\n",
        "  # process the output\n",
        "  predicted_order = generated_texts[0].strip()\n",
        "  predicted_order = predicted_order.split(\"Assistant:\")[-1].strip()\n",
        "  predicted_order = [name.strip() for name in predicted_order.split(\",\")]\n",
        "\n",
        "  expected_order = entry['order']\n",
        "\n",
        "  if predicted_order == expected_order:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "\n",
        "print(f\"Accuracy: {correct / total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WwzTrBuXU1bR",
        "outputId": "a225a40b-582f-497f-eca7-69cd2b595ab4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# If you have a `dataset/` folder, list that too\n",
        "dataset_path = '/content/drive/My Drive/CS231N Colabs/dataset'\n",
        "os.listdir(dataset_path)\n",
        "print(\"image_0.png\" in os.listdir(dataset_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geCuQuBdKc1l"
      },
      "source": [
        "# Baseline Supervised Fine Tuning w/ Cross Entropy Loss\n",
        "\n",
        "CS231N Colabs/dataset - 2500 images\n",
        "2000 train, 250 val, 250 test\n",
        "\n",
        "0-1999, 2000-2249, 2250-2499\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assume `processor` and `model` are already defined (and moved to DEVICE).\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#######################################\n",
        "# 1) Define a custom Dataset class  ##\n",
        "#######################################\n",
        "\n",
        "class VLMOrderDataset(Dataset):\n",
        "    def __init__(self, metadata_json_path, base_image_dir, processor, max_length=128):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            metadata_json_path (str): path to metadata.json\n",
        "            base_image_dir (str): root folder where images live\n",
        "            processor (AutoProcessor): the Hugging Face processor for SmolVLM\n",
        "            max_length (int): maximum token length for textual sequences\n",
        "        \"\"\"\n",
        "        with open(metadata_json_path, 'r') as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        self.base_image_dir = base_image_dir\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Fixed question text (same for every example):\n",
        "        self.question_text = (\n",
        "            \"Given the diagram, list the labels of the circles in order from leftmost to rightmost \"\n",
        "            \"(provide name only).\"\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "        image_rel_path = entry[\"image_path\"]\n",
        "        image_rel_path = image_rel_path.replace(\"dataset/\", \"\")\n",
        "        order_list = entry[\"order\"]  # e.g. [\"Hannah\",\"Grace\",\"Alice\",\"Emily\",\"David\"]\n",
        "\n",
        "        # 1) Load the image from disk:\n",
        "        image_path = os.path.join(self.base_image_dir, image_rel_path)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # 2) Build the “prompt” exactly as at inference:\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\"},\n",
        "                    {\"type\": \"text\", \"text\": self.question_text}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "        prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "        # 3) Convert the target list into a comma‐separated string:\n",
        "        #    e.g. \"Hannah, Grace, Alice, Emily, David\"\n",
        "        target_text = \", \".join(order_list)\n",
        "\n",
        "        # 4) Tokenize both with truncation disabled so that the <image> token is never dropped:\n",
        "        encoding = self.processor(\n",
        "            text=prompt,\n",
        "            images=[image],\n",
        "            text_target=target_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=False,       # ← disable truncation for the input\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "        # encoding[\"input_ids\"] is shape (1, seq_len), same for attention_mask & labels;\n",
        "        # encoding[\"pixel_values\"] is shape (1, 3, H, W).\n",
        "        # We .squeeze(0) to make them 1D/3D instead of batch‐size=1.\n",
        "        pixel_values = encoding[\"pixel_values\"].squeeze(0)     # (3, H, W)\n",
        "        input_ids     = encoding[\"input_ids\"].squeeze(0)       # (seq_len,)\n",
        "        attention_mask= encoding[\"attention_mask\"].squeeze(0)  # (seq_len,)\n",
        "        labels        = encoding[\"labels\"].squeeze(0)          # (target_seq_len,)\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"input_ids\":     input_ids,\n",
        "            \"attention_mask\":attention_mask,\n",
        "            \"labels\":        labels\n",
        "        }\n",
        "\n",
        "###############################################\n",
        "# 2) Instantiate train/val splits & DataLoaders ##\n",
        "###############################################\n",
        "\n",
        "# Paths—adjust to wherever you mounted your drive in Colab:\n",
        "base_drive_path   = \"/content/drive/My Drive/CS231N Colabs\"\n",
        "metadata_path     = os.path.join(base_drive_path, \"dataset/metadata.json\")\n",
        "images_base_dir = os.path.join(base_drive_path, \"dataset\", \"images\")\n",
        "\n",
        "# Load full dataset\n",
        "full_dataset = VLMOrderDataset(\n",
        "    metadata_json_path=metadata_path,\n",
        "    base_image_dir=images_base_dir,\n",
        "    processor=processor,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "# Split indices: 0–1999 = train, 2000–2249 = val, 2250–2499 = test\n",
        "num_examples = len(full_dataset)  # should be 2500\n",
        "assert num_examples == 2500, \"Check that metadata.json has 2500 entries.\"\n",
        "\n",
        "train_indices = list(range(0, 2000))\n",
        "val_indices   = list(range(2000, 2250))\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "train_dataset = Subset(full_dataset, train_indices)\n",
        "val_dataset   = Subset(full_dataset, val_indices)\n",
        "\n",
        "# DataLoader (you can adjust batch_size as GPU allows; 4–8 is a good starting point)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "###################################################\n",
        "# 3) Set up optimizer, training loop, and metrics ##\n",
        "###################################################\n",
        "\n",
        "#  We’ll use AdamW and a small learning rate. Feel free to tune later.\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "num_epochs = 3   # start small; you can increase if you have time\n",
        "\n",
        "# (Optional) learning‐rate scheduler, gradient clipping, etc. can be added here.\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_steps = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} / {num_epochs} (train)\", leave=False)\n",
        "    for batch in pbar:\n",
        "        # Move everything to DEVICE\n",
        "        pixel_values  = batch[\"pixel_values\"].to(DEVICE)      # (B, 3, H, W)\n",
        "        input_ids      = batch[\"input_ids\"].to(DEVICE)        # (B, seq_len)\n",
        "        attention_mask = batch[\"attention_mask\"].to(DEVICE)   # (B, seq_len)\n",
        "        labels         = batch[\"labels\"].to(DEVICE)           # (B, target_len)\n",
        "\n",
        "        # Forward pass: since we passed `labels`, HF will compute cross‐entropy internally\n",
        "        outputs = model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # (Optional) torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_steps += 1\n",
        "        pbar.set_postfix_str(f\"loss={train_loss/train_steps:.4f}\")\n",
        "\n",
        "    avg_train_loss = train_loss / train_steps\n",
        "    print(f\"\\nEpoch {epoch+1} — Avg train loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 4) Validate after each epoch\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_steps = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} / {num_epochs} (val)\", leave=False):\n",
        "            pixel_values  = batch[\"pixel_values\"].to(DEVICE)\n",
        "            input_ids      = batch[\"input_ids\"].to(DEVICE)\n",
        "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "            labels         = batch[\"labels\"].to(DEVICE)\n",
        "\n",
        "            outputs = model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "            )\n",
        "            val_loss += outputs.loss.item()\n",
        "            val_steps += 1\n",
        "\n",
        "    avg_val_loss = val_loss / val_steps\n",
        "    print(f\"Epoch {epoch+1} — Avg val loss: {avg_val_loss:.4f}\\n\")\n",
        "\n",
        "    # (Optional) Save the best‐performing checkpoint\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        save_path = \"./best_smolvlm_baseline.pt\"\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"  → New best model saved at epoch {epoch+1} (val loss {avg_val_loss:.4f})\\n\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "EcOhlnpWOULL",
        "outputId": "05d6ecb8-6f1b-4488-890c-56efeb2355e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 408.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 118.88 MiB is free. Process 25236 has 39.43 GiB memory in use. Of the allocated memory 38.82 GiB is allocated by PyTorch, and 119.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6254a0b8fe6e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Forward pass: since we passed `labels`, HF will compute cross‐entropy internally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         outputs = model(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/modeling_idefics3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_attention_mask, image_hidden_states, labels, use_cache, output_attentions, output_hidden_states, cache_position, return_dict, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/modeling_idefics3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_attention_mask, image_hidden_states, use_cache, output_attentions, output_hidden_states, cache_position, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both pixel_values and image_hidden_states at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpixel_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m             \u001b[0mimage_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mimage_hidden_states\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0mimage_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/modeling_idefics3.py\u001b[0m in \u001b[0;36mget_image_features\u001b[0;34m(self, pixel_values, pixel_attention_mask)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;31m# Get sequence from the vision encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0mimage_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0mimage_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/modeling_idefics3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, patch_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0mpatch_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_4d_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    596\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/modeling_idefics3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs_embeds, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    418\u001b[0m                 )\n\u001b[1;32m    419\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m                 layer_outputs = encoder_layer(\n\u001b[0m\u001b[1;32m    421\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/modeling_idefics3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/modeling_idefics3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/activations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapproximate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 408.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 118.88 MiB is free. Process 25236 has 39.43 GiB memory in use. Of the allocated memory 38.82 GiB is allocated by PyTorch, and 119.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkMCJX-JKcGw"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install dependencies (if not already installed)\n",
        "!pip install --quiet hf_xet flash-attn\n",
        "!pip install --quiet transformers torch pillow tqdm\n",
        "!pip install --quiet torch torchvision transformers pillow tqdm\n",
        "!pip install --quiet --upgrade torch torchvision transformers pillow tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from torch.optim import AdamW\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "1EeQwtytIvNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and Paths\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "\n",
        "# Device selection\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ----------------------------\n",
        "# Adjust these two paths:\n",
        "# ----------------------------\n",
        "# 1) Where your images live (e.g. image_0.png, image_1.png, ...)\n",
        "IMAGE_DIR = \"/content/drive/My Drive/CS231N Colabs/dataset\"\n",
        "\n",
        "# 2) Where your metadata.json lives\n",
        "METADATA_PATH = \"/content/drive/My Drive/CS231N Colabs/dataset/metadata.json\"\n",
        "\n",
        "# # 3) (Optional) A place to save checkpoints\n",
        "# CHECKPOINT_DIR = \"/content/checkpoints\"\n",
        "# os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Print confirmations\n",
        "print(\"Device:\", DEVICE)\n",
        "print(\"Image directory:\", IMAGE_DIR)\n",
        "print(\"Metadata path:\", METADATA_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40fIKGqUQTdZ",
        "outputId": "322b3b93-35c6-459c-f4be-8d3761ef49ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Image directory: /content/drive/My Drive/CS231N Colabs/dataset\n",
            "Metadata path: /content/drive/My Drive/CS231N Colabs/dataset/metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 – Load the pre‐trained processor & model\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
        "model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\").to(DEVICE)\n",
        "print(\"✓ Loaded SmolVLM‐Instruct\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYo8O6M8QawX",
        "outputId": "f28d2742-968f-4d95-aec7-f2ba803b0900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded SmolVLM‐Instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 – Define Dataset that returns raw PIL images + chat‐prompt + target string\n",
        "class CircleOrderDataset(Dataset):\n",
        "    def __init__(self, metadata_list, image_dir, processor):\n",
        "        self.entries   = metadata_list\n",
        "        self.image_dir = image_dir\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.entries[idx]\n",
        "\n",
        "        # 1) Load image\n",
        "        img_name = os.path.basename(entry[\"image_path\"])\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image    = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # 2) Build chat‐style prompt (one <image> token)\n",
        "        question = \"Given the diagram, list the labels of the circles in order from leftmost to rightmost.\"\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\"},\n",
        "                    {\"type\": \"text\", \"text\": question}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "        chat_prompt = self.processor.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "\n",
        "        # 3) Build target (comma‐separated)\n",
        "        target_text = \", \".join(entry[\"order\"])\n",
        "\n",
        "        return {\n",
        "            \"image\":       image,\n",
        "            \"chat_prompt\": chat_prompt,  # contains one <image> token\n",
        "            \"target_text\": target_text\n",
        "        }\n"
      ],
      "metadata": {
        "id": "uy7EtOw7Qd-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 – Load metadata.json & split into train/val/test lists\n",
        "with open(METADATA_PATH, \"r\") as f:\n",
        "    all_metadata = json.load(f)\n",
        "\n",
        "assert len(all_metadata) == 2500, f\"Expected 2500 entries, got {len(all_metadata)}\"\n",
        "\n",
        "# Split indices\n",
        "train_metadata = all_metadata[0:2000]    # 2000 for training\n",
        "val_metadata   = all_metadata[2000:2250] # 250 for validation\n",
        "test_metadata  = all_metadata[2250:2500] # 250 for testing\n",
        "\n",
        "# Instantiate Datasets\n",
        "train_dataset = CircleOrderDataset(train_metadata, IMAGE_DIR, processor)\n",
        "val_dataset   = CircleOrderDataset(val_metadata,   IMAGE_DIR, processor)\n",
        "test_dataset  = CircleOrderDataset(test_metadata,  IMAGE_DIR, processor)\n",
        "\n",
        "print(\"✓ Train size:\", len(train_dataset))\n",
        "print(\"✓ Val   size:\", len(val_dataset))\n",
        "print(\"✓ Test  size:\", len(test_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk66X28QQh2q",
        "outputId": "c49d61ea-e013-4740-9d66-cd7eef1c016c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Train size: 2000\n",
            "✓ Val   size: 250\n",
            "✓ Test  size: 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_collator(batch):\n",
        "    \"\"\"\n",
        "    batch: list of dicts, each with keys:\n",
        "      - \"image\"       : PIL.Image\n",
        "      - \"chat_prompt\" : str (contains exactly one <image> token)\n",
        "      - \"target_text\" : str (comma‐separated labels)\n",
        "\n",
        "    Returns:\n",
        "      {\n",
        "        pixel_values:   Tensor (B, 1, 3, H, W),\n",
        "        input_ids:      Tensor (B, L1),\n",
        "        attention_mask: Tensor (B, L1),\n",
        "        labels:         Tensor (B, L2) with pad_token_id → -100\n",
        "      }\n",
        "    \"\"\"\n",
        "    images       = [item[\"image\"]       for item in batch]\n",
        "    chat_prompts = [item[\"chat_prompt\"] for item in batch]\n",
        "    targets      = [item[\"target_text\"] for item in batch]\n",
        "\n",
        "    # 1) Multimodal tokenization: text with <image> + images\n",
        "    model_inputs = processor(\n",
        "        text=chat_prompts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,       # pad to longest in batch\n",
        "        truncation=True     # truncate if too long\n",
        "    )\n",
        "    # model_inputs.pixel_values has shape (B, 1, C, H, W)—do NOT squeeze!\n",
        "\n",
        "    pixel_values   = model_inputs.pixel_values    # (B, 1, 3, H, W)\n",
        "    input_ids      = model_inputs.input_ids       # (B, L1)\n",
        "    attention_mask = model_inputs.attention_mask  # (B, L1)\n",
        "\n",
        "    # 2) Tokenize & pad the target strings\n",
        "    label_encodings = processor.tokenizer(\n",
        "        targets,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )[\"input_ids\"]  # (B, L2)\n",
        "\n",
        "    # 3) Replace pad_token_id with -100 so loss ignores those positions\n",
        "    pad_id = processor.tokenizer.pad_token_id\n",
        "    label_encodings[label_encodings == pad_id] = -100\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\":   pixel_values.to(DEVICE),     # (B, 1, 3, H, W)\n",
        "        \"input_ids\":      input_ids.to(DEVICE),        # (B, L1)\n",
        "        \"attention_mask\": attention_mask.to(DEVICE),   # (B, L1)\n",
        "        \"labels\":         label_encodings.to(DEVICE),  # (B, L2)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "AfifDWseRDnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 – Create DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "print(\"✓ # train batches:\", len(train_loader))\n",
        "print(\"✓ # val   batches:\", len(val_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQroHQLzRF2l",
        "outputId": "4b4e6a49-b879-4692-db70-55c79638ac43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ # train batches: 2000\n",
            "✓ # val   batches: 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Immediately after loading processor, shrink images to 128×128\n",
        "processor.image_processor.size = {\"height\": 128, \"width\": 128}\n",
        "\n",
        "# 2) Load the model and enable checkpointing\n",
        "model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\").to(DEVICE)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# 3) In your DataLoaders, use batch_size=1\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=1, shuffle=False, collate_fn=data_collator)\n",
        "\n",
        "# 4) Use mixed precision + smaller prompt length in collator\n",
        "#    (modify your collator to include max_length=128 if desired)\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "scaler = GradScaler()\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            outputs = model(\n",
        "                pixel_values=batch[\"pixel_values\"],\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                labels=batch[\"labels\"],\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} train loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    total_val = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "            with autocast():\n",
        "                outputs = model(\n",
        "                    pixel_values=batch[\"pixel_values\"],\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    labels=batch[\"labels\"],\n",
        "                )\n",
        "                total_val += outputs.loss.item()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} val   loss: {total_val/len(val_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "RdSnRhUhRLF5",
        "outputId": "4621d1e7-6192-4ad1-e0a2-152fe0862375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 16.88 MiB is free. Process 9658 has 39.53 GiB memory in use. Of the allocated memory 38.27 GiB is allocated by PyTorch, and 779.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-87e3b8c0400b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 2) Load the model and enable checkpointing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForVision2Seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HuggingFaceTB/SmolVLM-Instruct\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing_enable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3848\u001b[0m                     \u001b[0;34m\"You cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3849\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3850\u001b[0;31m                 )\n\u001b[0m\u001b[1;32m   3851\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                 return t.to(\n\u001b[1;32m   1342\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m                     \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m         should_use_swap_tensors = (\n\u001b[1;32m    932\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__future__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_swap_module_params_on_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                     \u001b[0;34m\"and some modules might not work as expected when using complex tensors as parameters or buffers. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;34m\"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                     \u001b[0;34m\"if a complex module does not work as expected.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m                 )\n\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 16.88 MiB is free. Process 9658 has 39.53 GiB memory in use. Of the allocated memory 38.27 GiB is allocated by PyTorch, and 779.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# (Assume processor, model, train_loader, val_loader, optimizer, etc. are already defined)\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(3):\n",
        "    # ----- Training -----\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 1) Move all inputs to the same device\n",
        "        pixel_values   = batch[\"pixel_values\"].to(DEVICE)\n",
        "        input_ids      = batch[\"input_ids\"].to(DEVICE)\n",
        "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "        labels         = batch[\"labels\"].to(DEVICE)\n",
        "\n",
        "        # 2) Now run the forward pass under autocast\n",
        "        with autocast():\n",
        "            outputs = model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 2) Delete intermediate tensors to break Python references\n",
        "        del outputs, loss\n",
        "\n",
        "        # 3) Force garbage collection, then free any unused cached GPU memory\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} — Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # ----- Validation -----\n",
        "    model.eval()\n",
        "    total_val = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in val_loader:\n",
        "          pixel_values   = batch[\"pixel_values\"].to(DEVICE)\n",
        "          input_ids      = batch[\"input_ids\"].to(DEVICE)\n",
        "          attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "          labels         = batch[\"labels\"].to(DEVICE)\n",
        "\n",
        "          with autocast():\n",
        "              outputs = model(\n",
        "                  pixel_values=pixel_values,\n",
        "                  input_ids=input_ids,\n",
        "                  attention_mask=attention_mask,\n",
        "                  labels=labels,\n",
        "              )\n",
        "              val_loss = outputs.loss\n",
        "\n",
        "    avg_val_loss = total_val / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1} — Avg Val Loss: {avg_val_loss:.4f}\\n\")\n",
        "\n",
        "    # Optionally, you can also call empty_cache once more here:\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512,
          "referenced_widgets": [
            "bd104e45890a46abb9678161aba15469",
            "2b0d446db0394741ab21191dc83598d6",
            "d4e8bc787fef467e96d2b09e0261bd87",
            "97a690b5f566409eac3876e18ffce5ef",
            "d77a665cfaf54465946bccbbbb187e70",
            "196788b5a7ea4ad29a95a56f3f42608c",
            "a9b7199d134b4351894eac35b2dde5f1",
            "e57edc5b338842a8b04434abd9b760ee",
            "cc7010c26f2f499e9443676d3a8a4690",
            "b3d82a2e7c724075829a36c8078258e9",
            "4b119556244748a8832f42d2afeb6d5a"
          ]
        },
        "id": "RXBriGuiahfh",
        "outputId": "eedd5b2e-4ed0-492c-c417-65c33c825161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-15db9d14d6df>:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training Epoch 1:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd104e45890a46abb9678161aba15469"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-15db9d14d6df>:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-15db9d14d6df>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# 2) Now run the forward pass under autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             outputs = model(\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1750\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/modeling_idefics3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_attention_mask, image_hidden_states, labels, use_cache, output_attentions, output_hidden_states, cache_position, return_dict, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1750\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/modeling_idefics3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_attention_mask, image_hidden_states, use_cache, output_attentions, output_hidden_states, cache_position, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0;31m# START VISUAL INPUTS INTEGRATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mtracing_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1750\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell X: Verify that all inputs and model parameters are on the same device\n",
        "\n",
        "import torch\n",
        "\n",
        "# Grab one batch from the training DataLoader\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "# 1) Print devices of raw batch tensors\n",
        "print(\"Raw batch tensor devices:\")\n",
        "print(\"  pixel_values:   \", batch[\"pixel_values\"].device)\n",
        "print(\"  input_ids:      \", batch[\"input_ids\"].device)\n",
        "print(\"  attention_mask: \", batch[\"attention_mask\"].device)\n",
        "print(\"  labels:         \", batch[\"labels\"].device)\n",
        "\n",
        "# 2) Move each tensor to the target DEVICE\n",
        "pixel_values   = batch[\"pixel_values\"].to(DEVICE)\n",
        "input_ids      = batch[\"input_ids\"].to(DEVICE)\n",
        "attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "labels         = batch[\"labels\"].to(DEVICE)\n",
        "\n",
        "# 3) Print devices after .to(DEVICE)\n",
        "print(\"\\nAfter .to(DEVICE):\")\n",
        "print(\"  pixel_values:   \", pixel_values.device)\n",
        "print(\"  input_ids:      \", input_ids.device)\n",
        "print(\"  attention_mask: \", attention_mask.device)\n",
        "print(\"  labels:         \", labels.device)\n",
        "\n",
        "# 4) Check every model parameter’s device\n",
        "print(\"\\nModel parameter devices (non-GPU parameters, if any):\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.device != torch.device(DEVICE):\n",
        "        print(f\"  ⚠️  {name} is on {param.device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c1onDCHXj1X",
        "outputId": "cf90c249-35f3-4895-f966-3b45cfc3b0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw batch tensor devices:\n",
            "  pixel_values:    cuda:0\n",
            "  input_ids:       cuda:0\n",
            "  attention_mask:  cuda:0\n",
            "  labels:          cuda:0\n",
            "\n",
            "After .to(DEVICE):\n",
            "  pixel_values:    cuda:0\n",
            "  input_ids:       cuda:0\n",
            "  attention_mask:  cuda:0\n",
            "  labels:          cuda:0\n",
            "\n",
            "Model parameter devices (non-GPU parameters, if any):\n",
            "  ⚠️  model.vision_model.embeddings.patch_embedding.weight is on cpu\n",
            "  ⚠️  model.vision_model.embeddings.patch_embedding.bias is on cpu\n",
            "  ⚠️  model.vision_model.embeddings.position_embedding.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.0.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.1.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.2.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.3.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.4.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.5.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.6.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.7.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.8.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.9.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.10.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.11.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.12.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.13.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.14.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.15.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.16.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.17.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.18.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.19.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.20.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.21.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.22.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.23.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.24.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.25.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.self_attn.k_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.self_attn.v_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.self_attn.q_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.self_attn.out_proj.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.self_attn.out_proj.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.layer_norm1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.layer_norm1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.mlp.fc1.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.mlp.fc1.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.mlp.fc2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.mlp.fc2.bias is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.layer_norm2.weight is on cpu\n",
            "  ⚠️  model.vision_model.encoder.layers.26.layer_norm2.bias is on cpu\n",
            "  ⚠️  model.vision_model.post_layernorm.weight is on cpu\n",
            "  ⚠️  model.vision_model.post_layernorm.bias is on cpu\n",
            "  ⚠️  model.connector.modality_projection.proj.weight is on cpu\n",
            "  ⚠️  model.text_model.embed_tokens.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.0.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.0.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.0.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.0.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.0.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.0.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.0.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.0.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.0.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.1.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.1.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.1.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.1.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.1.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.1.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.1.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.1.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.1.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.2.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.2.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.2.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.2.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.2.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.2.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.2.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.2.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.2.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.3.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.3.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.3.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.3.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.3.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.3.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.3.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.3.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.3.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.4.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.4.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.4.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.4.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.4.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.4.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.4.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.4.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.4.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.5.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.5.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.5.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.5.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.5.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.5.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.5.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.5.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.5.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.6.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.6.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.6.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.6.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.6.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.6.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.6.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.6.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.6.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.7.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.7.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.7.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.7.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.7.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.7.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.7.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.7.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.7.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.8.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.8.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.8.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.8.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.8.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.8.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.8.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.8.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.8.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.9.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.9.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.9.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.9.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.9.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.9.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.9.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.9.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.9.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.10.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.10.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.10.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.10.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.10.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.10.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.10.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.10.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.10.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.11.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.11.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.11.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.11.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.11.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.11.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.11.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.11.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.11.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.12.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.12.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.12.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.12.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.12.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.12.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.12.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.12.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.12.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.13.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.13.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.13.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.13.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.13.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.13.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.13.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.13.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.13.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.14.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.14.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.14.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.14.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.14.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.14.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.14.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.14.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.14.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.15.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.15.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.15.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.15.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.15.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.15.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.15.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.15.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.15.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.16.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.16.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.16.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.16.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.16.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.16.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.16.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.16.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.16.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.17.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.17.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.17.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.17.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.17.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.17.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.17.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.17.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.17.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.18.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.18.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.18.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.18.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.18.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.18.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.18.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.18.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.18.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.19.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.19.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.19.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.19.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.19.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.19.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.19.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.19.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.19.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.20.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.20.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.20.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.20.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.20.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.20.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.20.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.20.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.20.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.21.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.21.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.21.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.21.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.21.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.21.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.21.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.21.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.21.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.22.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.22.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.22.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.22.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.22.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.22.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.22.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.22.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.22.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.23.self_attn.q_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.23.self_attn.k_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.23.self_attn.v_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.23.self_attn.o_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.23.mlp.gate_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.23.mlp.up_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.23.mlp.down_proj.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.23.input_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.layers.23.post_attention_layernorm.weight is on cpu\n",
            "  ⚠️  model.text_model.norm.weight is on cpu\n",
            "  ⚠️  lm_head.weight is on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell Z: Move the entire model (all submodules) to GPU\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Verify that no parameters remain on CPU\n",
        "import torch\n",
        "for name, param in model.named_parameters():\n",
        "    if param.device != torch.device(DEVICE):\n",
        "        print(f\"⚠️ {name} is still on {param.device}\")\n",
        "\n",
        "print(\"✓ All model parameters are now on\", DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "ohMnUNHpW-0V",
        "outputId": "8539ae38-590f-4263-8a96-826822a5f12a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 14.88 MiB is free. Process 9658 has 39.53 GiB memory in use. Of the allocated memory 38.28 GiB is allocated by PyTorch, and 772.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-54f0ff514c6e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cell Z: Move the entire model (all submodules) to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Verify that no parameters remain on CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3848\u001b[0m                     \u001b[0;34m\"You cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3849\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3850\u001b[0;31m                 )\n\u001b[0m\u001b[1;32m   3851\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                 return t.to(\n\u001b[1;32m   1342\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m                     \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExtra\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m         should_use_swap_tensors = (\n\u001b[1;32m    932\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__future__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_swap_module_params_on_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                     \u001b[0;34m\"and some modules might not work as expected when using complex tensors as parameters or buffers. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;34m\"Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                     \u001b[0;34m\"if a complex module does not work as expected.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m                 )\n\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 14.88 MiB is free. Process 9658 has 39.53 GiB memory in use. Of the allocated memory 38.28 GiB is allocated by PyTorch, and 772.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "\n",
        "# 1) Force‐free any leftover GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# 2) Check that GPU is truly empty\n",
        "print(\"Free CUDA memory before loading:\",\n",
        "      torch.cuda.mem_get_info()[0] / (1024**3), \"GB\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 3) Load processor and shrink images to 32×32 (tiny)\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
        "processor.image_processor.size = {\"height\": 32, \"width\": 32}\n",
        "\n",
        "# 4) Load model in FP16, enable checkpointing, and move to GPU\n",
        "model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\").half().to(DEVICE)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# 5) Immediately delete everything except the model and processor\n",
        "del processor\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 6) Check GPU usage now\n",
        "print(\"Free CUDA memory after loading:\",\n",
        "      torch.cuda.mem_get_info()[0] / (1024**3), \"GB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "I-zcQhD6cTc0",
        "outputId": "7511990b-7c13-4478-b7f1-102830d66951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free CUDA memory before loading: 39.1434326171875 GB\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "/usr/local/lib/python3.11/dist-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5a865f7d04d3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 4) Load model in FP16, enable checkpointing, and move to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForVision2Seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HuggingFaceTB/SmolVLM-Instruct\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing_enable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0msupported_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupported_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msupported_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_attr_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;31m# Maybe there was several model types associated with this config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{module_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformers.models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;31m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2075\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2071\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2073\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/idefics3/modeling_idefics3.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_attn_mask_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_prepare_4d_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_flash_attention_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlashAttentionKwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_outputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModelOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_flash_attention_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_flash_attn_2_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mflash_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_padding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mindex_first_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpad_input\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mflash_attn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflash_attn_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflash_attn_varlen_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mflash_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_rotary_emb\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flash_attn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.7.4.post1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m from flash_attn.flash_attn_interface import (\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mflash_attn_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mflash_attn_kvpacked_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flash_attn/flash_attn_interface.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mflash_attn_triton_amd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterface_fa\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mflash_attn_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mflash_attn_2_cuda\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mflash_attn_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# isort: on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.11/dist-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, uninstall any existing flash‐attn that may be incompatible:\n",
        "!pip uninstall -y flash-attn\n",
        "\n",
        "# Clone the official FlashAttention repo and build it against the current PyTorch:\n",
        "!git clone https://github.com/Dao-AILab/flash-attention.git\n",
        "%cd flash-attention\n",
        "!pip install .\n",
        "\n",
        "# Go back to your notebook root and clear cache\n",
        "%cd ..\n",
        "import torch, gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Now retry loading SmolVLM\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
        "processor.image_processor.size = {\"height\": 64, \"width\": 64}\n",
        "\n",
        "model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\").half().to(DEVICE)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"✅ FlashAttention built and model loaded on:\", DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hdH-6sDRdQx2",
        "outputId": "b3156eee-37bd-43d7-8140-4ea65a81ca4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: flash_attn 2.7.4.post1\n",
            "Uninstalling flash_attn-2.7.4.post1:\n",
            "  Successfully uninstalled flash_attn-2.7.4.post1\n",
            "Cloning into 'flash-attention'...\n",
            "remote: Enumerating objects: 9732, done.\u001b[K\n",
            "remote: Counting objects: 100% (117/117), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 9732 (delta 73), reused 49 (delta 38), pack-reused 9615 (from 3)\u001b[K\n",
            "Receiving objects: 100% (9732/9732), 9.49 MiB | 26.28 MiB/s, done.\n",
            "Resolving deltas: 100% (7507/7507), done.\n",
            "/content/flash-attention\n",
            "Processing /content/flash-attention\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash_attn==2.7.4.post1) (2.7.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash_attn==2.7.4.post1) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn==2.7.4.post1) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch->flash_attn==2.7.4.post1) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->flash_attn==2.7.4.post1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash_attn==2.7.4.post1) (3.0.2)\n",
            "Building wheels for collected packages: flash_attn\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25h/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7a47af4182c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d687bb1fa079>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"height\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"width\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForVision2Seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HuggingFaceTB/SmolVLM-Instruct\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing_enable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4572\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4573\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4574\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4575\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4576\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5029\u001b[0m             \u001b[0;31m# Skip it with fsdp on ranks other than 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5030\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5031\u001b[0;31m                 disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   5032\u001b[0m                     \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5033\u001b[0m                     \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcasting_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasting_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mto_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Train SmolVLM‐Instruct on CPU only (no CUDA)\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "\n",
        "# 1) Force everything onto CPU\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "# 2) Load the processor and do a small image resize (e.g. 128×128)\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
        "processor.image_processor.size = {\"height\": 128, \"width\": 128}\n",
        "\n",
        "# 3) Define Dataset (returns PIL images, chat‐style prompts, target texts)\n",
        "class CircleOrderDataset(Dataset):\n",
        "    def __init__(self, metadata_list, image_dir, processor):\n",
        "        self.entries   = metadata_list\n",
        "        self.image_dir = image_dir\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.entries[idx]\n",
        "        img_name = os.path.basename(entry[\"image_path\"])\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image    = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        question = \"Given the diagram, list the labels of the circles in order from leftmost to rightmost.\"\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": question}]}\n",
        "        ]\n",
        "        chat_prompt = self.processor.apply_chat_template(messages, add_generation_prompt=False)\n",
        "        target_text = \", \".join(entry[\"order\"])\n",
        "        return {\"image\": image, \"chat_prompt\": chat_prompt, \"target_text\": target_text}\n",
        "\n",
        "# 4) Data collator: tokenize on CPU (no .to(cuda))\n",
        "def data_collator(batch):\n",
        "    images       = [item[\"image\"] for item in batch]\n",
        "    chat_prompts = [item[\"chat_prompt\"] for item in batch]\n",
        "    targets      = [item[\"target_text\"] for item in batch]\n",
        "\n",
        "    # Encode text+image together\n",
        "    model_inputs = processor(\n",
        "        text=chat_prompts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "    # model_inputs.pixel_values has shape (B, 1, 3, 128, 128) on CPU\n",
        "    pixel_values   = model_inputs.pixel_values   # (B,1,3,128,128)\n",
        "    input_ids      = model_inputs.input_ids      # (B, L1)\n",
        "    attention_mask = model_inputs.attention_mask # (B, L1)\n",
        "\n",
        "    # Tokenize and pad targets\n",
        "    label_encodings = processor.tokenizer(\n",
        "        targets,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )[\"input_ids\"]  # (B, L2)\n",
        "    pad_id = processor.tokenizer.pad_token_id\n",
        "    label_encodings[label_encodings == pad_id] = -100\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\":   pixel_values,\n",
        "        \"input_ids\":      input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\":         label_encodings,\n",
        "    }\n",
        "\n",
        "# 5) Load metadata.json and split\n",
        "BASE_DIR     = \"/content\"                         # adjust if needed\n",
        "IMAGE_DIR    = os.path.join(BASE_DIR, \"dataset\")  # folder with images\n",
        "METADATA_PATH = os.path.join(BASE_DIR, \"metadata.json\")\n",
        "\n",
        "with open(METADATA_PATH, \"r\") as f:\n",
        "    all_metadata = json.load(f)\n",
        "\n",
        "train_meta = all_metadata[0:2000]\n",
        "val_meta   = all_metadata[2000:2250]\n",
        "\n",
        "train_dataset = CircleOrderDataset(train_meta, IMAGE_DIR, processor)\n",
        "val_dataset   = CircleOrderDataset(val_meta,   IMAGE_DIR, processor)\n",
        "\n",
        "# 6) DataLoaders with batch_size=1 (keep small to reduce CPU RAM)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=data_collator)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=1, shuffle=False, collate_fn=data_collator)\n",
        "\n",
        "print(\"✓ Train batches:\", len(train_loader), \"| Val batches:\", len(val_loader))\n",
        "\n",
        "# 7) Load model on CPU and set up optimizer + loss\n",
        "model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\").to(DEVICE)\n",
        "model.gradient_checkpointing_enable()  # saves some CPU activation memory\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn   = CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "# 8) Simple training loop on CPU\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
        "        # Move inputs to CPU (they are already on CPU by default)\n",
        "        pixel_values   = batch[\"pixel_values\"]      # (1,1,3,128,128)\n",
        "        input_ids      = batch[\"input_ids\"]         # (1, L1)\n",
        "        attention_mask = batch[\"attention_mask\"]    # (1, L1)\n",
        "        labels         = batch[\"labels\"]            # (1, L2)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        # HuggingFace’s Vision2Seq returns outputs.loss when labels are provided\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} ▶ Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 9) Validation on CPU\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
        "            pixel_values   = batch[\"pixel_values\"]\n",
        "            input_ids      = batch[\"input_ids\"]\n",
        "            attention_mask = batch[\"attention_mask\"]\n",
        "            labels         = batch[\"labels\"]\n",
        "\n",
        "            outputs = model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            total_val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1} ▶ Avg Val Loss: {avg_val_loss:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "xKKdrbKufoIW",
        "outputId": "22e03b5e-1c67-4455-d7fc-2a7ceb782c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AutoProcessor' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-26a3e34f90f8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForVision2Seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 1) Force everything onto CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AutoProcessor' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd104e45890a46abb9678161aba15469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b0d446db0394741ab21191dc83598d6",
              "IPY_MODEL_d4e8bc787fef467e96d2b09e0261bd87",
              "IPY_MODEL_97a690b5f566409eac3876e18ffce5ef"
            ],
            "layout": "IPY_MODEL_d77a665cfaf54465946bccbbbb187e70"
          }
        },
        "2b0d446db0394741ab21191dc83598d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_196788b5a7ea4ad29a95a56f3f42608c",
            "placeholder": "​",
            "style": "IPY_MODEL_a9b7199d134b4351894eac35b2dde5f1",
            "value": "Training Epoch 1:   0%"
          }
        },
        "d4e8bc787fef467e96d2b09e0261bd87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e57edc5b338842a8b04434abd9b760ee",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc7010c26f2f499e9443676d3a8a4690",
            "value": 0
          }
        },
        "97a690b5f566409eac3876e18ffce5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3d82a2e7c724075829a36c8078258e9",
            "placeholder": "​",
            "style": "IPY_MODEL_4b119556244748a8832f42d2afeb6d5a",
            "value": " 0/2000 [00:00&lt;?, ?it/s]"
          }
        },
        "d77a665cfaf54465946bccbbbb187e70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "196788b5a7ea4ad29a95a56f3f42608c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b7199d134b4351894eac35b2dde5f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e57edc5b338842a8b04434abd9b760ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc7010c26f2f499e9443676d3a8a4690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3d82a2e7c724075829a36c8078258e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b119556244748a8832f42d2afeb6d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "538c7c2b14264201b480c558f10518a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4dc5c25d9f074345ae90958de28bd0b4",
              "IPY_MODEL_e4028365966c4825910a6093849d9e3c",
              "IPY_MODEL_834d437f8a6b4fdca7be2308c13bb6a9"
            ],
            "layout": "IPY_MODEL_14d0b67f3ab0402cba972afd1eacc8fc"
          }
        },
        "4dc5c25d9f074345ae90958de28bd0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ae7ecc881854dad8498b6196e1c56bd",
            "placeholder": "​",
            "style": "IPY_MODEL_df1b32aa29764b69b1e36795b833954e",
            "value": "Processing Images:  10%"
          }
        },
        "e4028365966c4825910a6093849d9e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5843d952c6b94746b30bd72525a87a07",
            "max": 2500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5747eebb43664ce6984d9db2f4dede6f",
            "value": 250
          }
        },
        "834d437f8a6b4fdca7be2308c13bb6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb00eec0cfb3461192e2546ae6147973",
            "placeholder": "​",
            "style": "IPY_MODEL_d81ef8e1b1d94f1c940af6eab73f5480",
            "value": " 250/2500 [00:17&lt;10:59:50, 17.60s/it]"
          }
        },
        "14d0b67f3ab0402cba972afd1eacc8fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae7ecc881854dad8498b6196e1c56bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df1b32aa29764b69b1e36795b833954e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5843d952c6b94746b30bd72525a87a07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5747eebb43664ce6984d9db2f4dede6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb00eec0cfb3461192e2546ae6147973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d81ef8e1b1d94f1c940af6eab73f5480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}